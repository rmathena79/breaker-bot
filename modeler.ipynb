{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca2471e-572c-4233-b18c-e8e03e8cf40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 20:49:48.478349: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 20:49:48.487797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-01 20:49:48.496334: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-01 20:49:48.498822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 20:49:48.506282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from credentials import CONNECTION_INFO\n",
    "from constants import *\n",
    "\n",
    "import encoders\n",
    "import db_connect\n",
    "import helpers\n",
    "import tf_helpers\n",
    "\n",
    "# Callbacks for use with TensorFlow\n",
    "from tf_helpers import modulo_output, modulo_distance_loss, modulo_distance_accuracy, modulo_rounded_accuracy, initialize_save_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088ab2d-0294-47b0-8528-98cbbbfb15bd",
   "metadata": {},
   "source": [
    "## Config\n",
    "This notebook has a lot of options to adjust, most of which are controlled here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9bdcd65-ceca-4b1a-921a-4544e38b37b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 64, 512, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENCODER = encoders.ENCODER_CAESAR\n",
    "CHUNK_SIZE = 256 # Was 512\n",
    "PROCESSING_UNITS = CHUNK_SIZE // 4 # Was // 4\n",
    "\n",
    "EXTRA_CHECKS = False # Whether to run some (potentially slow) debug checks\n",
    "\n",
    "INFER_TEXT = False # Text inferrence isn't really working. I don't know how to combine the time-distributed results.\n",
    "INFER_KEY = not INFER_TEXT\n",
    "\n",
    "if INFER_TEXT:\n",
    "    MAIN_ACCURACY_METRIC = \"mae\"\n",
    "    LOSS_METRIC = \"mean_squared_error\"\n",
    "    OUTPUT_SIZE = CHUNK_SIZE\n",
    "    OPTIMIZER = \"sgd\"\n",
    "else:\n",
    "    MAIN_ACCURACY_METRIC = \"mae\"\n",
    "    LOSS_METRIC = \"mae\"\n",
    "    OPTIMIZER = \"adamax\"    \n",
    "\n",
    "    if ENCODER == encoders.ENCODER_CAESAR:\n",
    "        OUTPUT_SIZE = 1\n",
    "    elif ENCODER == encoders.ENCODER_SUBST:\n",
    "        OUTPUT_SIZE = len(encoders.CHARSET)\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported encoder {ENCODER}\")\n",
    "\n",
    "ENCRYPTED_FILE_LIMIT = -1 # -1 to disable limit\n",
    "\n",
    "BASE_TRAIN_PCT = 0.75   # Start here. If train or test count would exceed the max, reduce it. Note 0.75 is the default.\n",
    "MAX_TRAIN_COUNT = -1 # -1 to disable; some setups start running out of memory around 100K\n",
    "MAX_TEST_COUNT =  -1 # -1 to disable\n",
    "SPLIT_SEED = 42\n",
    "\n",
    "LOAD_BEST_MODEL = False # If False, a new model will be created from scratch\n",
    "SAVE_BEST_MODEL = True\n",
    "BEST_PATH = './saved_models/best.keras'\n",
    "\n",
    "LOAD_SCALER = True # If true, load from disk. If False, calculate and save to disk.\n",
    "SCALER_PATH = helpers.get_recommended_scaler_path(ENCODER, CHUNK_SIZE)\n",
    "\n",
    "# Whether to run the tuner or the hard-coded network build code\n",
    "TUNE_NETWORK = False\n",
    "TUNE_QUICKLY = False # Set True to sanity check the model builder\n",
    "BUILD_NETWORK = not TUNE_NETWORK\n",
    "TRAIN_MODEL = BUILD_NETWORK and not LOAD_BEST_MODEL\n",
    "\n",
    "TUNER_DIRECTORY = \"tuner_projects\"\n",
    "TUNER_PROJECT_NAME = \"KT\"\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = int(max(32, round(256 * (512/CHUNK_SIZE)))) # Default is 32 -- going higher speeds things up a LOT, but may cause memory problems\n",
    "\n",
    "CHUNK_SIZE, PROCESSING_UNITS, BATCH_SIZE, OUTPUT_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ec896-6840-4696-bebd-c0419f8a1f69",
   "metadata": {},
   "source": [
    "# Data Retrieval and Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ab2e5-5e11-4812-91be-bb9a63acb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db_connect.DB(CONNECTION_INFO)\n",
    "\n",
    "with db.get_session() as session:\n",
    "    # Get database IDs for encoders and key types\n",
    "    (encoder_ids, key_type_id) = db.get_id_maps(session)\n",
    "\n",
    "    # Map source ID to plaintext file (1) details, and source ID to corresponding ciphertext files (1+) details\n",
    "    (sid_to_p, sid_to_c) = db.get_source_maps(session, ENCRYPTED_FILE_LIMIT, encoder_ids[ENCODER], test_only=False)\n",
    "\n",
    "    # Get the features (X, the cipher texts as offsets) and targets (y, either the plain texts as offsets OR the key).\n",
    "    (X, y_keys, y_texts) = db.get_features_and_targets(session, sid_to_p, sid_to_c, ENCODER, CHUNK_SIZE)\n",
    "\n",
    "X = np.array(X)\n",
    "if INFER_KEY:\n",
    "    y = np.array(y_keys)\n",
    "if INFER_TEXT:\n",
    "    y = np.array(y_texts)\n",
    "        \n",
    "len(sid_to_p), len(sid_to_c), X.shape, y.shape, sys.getsizeof(X), sys.getsizeof(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72d2c0-2cf4-4d4e-89af-46021e40bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging...\n",
    "\n",
    "all_plaintexts = \"\"\n",
    "all_ciphertexts = \"\"\n",
    "if EXTRA_CHECKS:\n",
    "    # Get ALL the texts in one big string, for debugging\n",
    "    for sid in sid_to_p:\n",
    "        all_plaintexts += helpers.read_text_file(sid_to_p[sid].path)\n",
    "        for c in sid_to_c[sid]:\n",
    "            all_ciphertexts += helpers.read_text_file(c.path)\n",
    "    \n",
    "    # Make sure specified text occurs somewhere in the texts.\n",
    "    # These raise exceptions if not found.\n",
    "    def check_in_plaintext(to_check: str):\n",
    "        if to_check not in all_plaintexts:\n",
    "            raise Exception(f\"Plaintext not found: {to_check}\")\n",
    "    \n",
    "    def check_in_ciphertext(to_check: str):\n",
    "        if to_check not in all_ciphertexts:\n",
    "            raise Exception(f\"Ciphertext not found: {to_check}\")\n",
    "    \n",
    "\n",
    "    checks = round( len(X) * 0.01)\n",
    "    print(f\"Checking {checks} strings\")\n",
    "    for _ in range(checks):\n",
    "        i = random.randint(0, len(X)-1)\n",
    "        check_in_plaintext(encoders.offsets_to_string(y_texts[i].astype(int)))\n",
    "        check_in_ciphertext(encoders.offsets_to_string(X[i].astype(int)))\n",
    "\n",
    "len(all_plaintexts), len(all_ciphertexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55451fa-ee72-4835-b18a-d03d44899412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Note we have excluded \"test_only\" files above, they will be used for later validation.\n",
    "\n",
    "train_count = int(round(len(y) * BASE_TRAIN_PCT))\n",
    "if train_count > MAX_TRAIN_COUNT and MAX_TRAIN_COUNT > -1:\n",
    "    print(f\"Train count would be {train_count}\")\n",
    "    train_count = int(MAX_TRAIN_COUNT)\n",
    "print(f\"Train count is {train_count}\")\n",
    "\n",
    "test_count = len(y) - train_count\n",
    "if test_count > MAX_TEST_COUNT and MAX_TEST_COUNT > -1:\n",
    "    print(f\"Test count would be {test_count}\")\n",
    "    test_count = int(MAX_TEST_COUNT)\n",
    "print(f\"Test count is {test_count}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_count, test_size=test_count, random_state=SPLIT_SEED)\n",
    "\n",
    "if EXTRA_CHECKS:\n",
    "    checks = max(10, round( min(len(X_train), len(X_test)) * 0.01))\n",
    "    print(f\"Checking {checks} strings\")\n",
    "    for _ in range(checks):\n",
    "        i = random.randint(0, len(X_train)-1)\n",
    "        check_in_ciphertext(encoders.offsets_to_string(X_train[i].astype(int)))\n",
    "\n",
    "        i = random.randint(0, len(X_test)-1)\n",
    "        check_in_ciphertext(encoders.offsets_to_string(X_test[i].astype(int)))\n",
    "\n",
    "# The pre-split data sets are no longer needed, and take up a lot of memory, so get rid of them\n",
    "if not EXTRA_CHECKS:\n",
    "    del X\n",
    "    del y\n",
    "    del y_keys\n",
    "    del y_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fd198-2be6-4f66-91ff-0bbb1ccb819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "\n",
    "if LOAD_SCALER:\n",
    "    print(f\"Loading scaler from {SCALER_PATH}\")\n",
    "    X_scaler = helpers.load_scaler_from_file(SCALER_PATH)\n",
    "else:\n",
    "    # Fit the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Fitting scaler\")\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "    \n",
    "    # Scale the data\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)   \n",
    "\n",
    "    print(f\"Saving scaler to {SCALER_PATH}\")\n",
    "    helpers.save_scaler_to_file(X_scaler, SCALER_PATH)\n",
    "    \n",
    "to_show = min(16, CHUNK_SIZE)\n",
    "X_train_scaled.shape, X_test_scaled.shape, X_train_scaled[0][0:to_show], X_test_scaled[0][0:to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa6ffe-b96c-49ff-abe9-858cd87dabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data as required for the model\n",
    "\n",
    "print(f\"Original shapes: {X_train.shape}, {X_test.shape}, {y_train.shape}, {y_test.shape}\")\n",
    "\n",
    "X_train = tf_helpers.reshape_input(X_train, CHUNK_SIZE)\n",
    "X_train_scaled = tf_helpers.reshape_input(X_train_scaled, CHUNK_SIZE)\n",
    "X_test = tf_helpers.reshape_input(X_test, CHUNK_SIZE)\n",
    "X_test_scaled = tf_helpers.reshape_input(X_test_scaled, CHUNK_SIZE)\n",
    "y_train = tf_helpers.reshape_output(y_train, OUTPUT_SIZE)\n",
    "y_test = tf_helpers.reshape_output(y_test, OUTPUT_SIZE)\n",
    "\n",
    "print(f\"Final    shapes: {X_train.shape}, {X_train_scaled.shape}, {X_test.shape}, {X_test_scaled.shape}, {y_train.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340dfc6-d8e6-461c-87af-f2627f638908",
   "metadata": {},
   "source": [
    "# Hyperband Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab864b-b88b-4dac-b504-c58c4fbdc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_tuner\n",
    "\n",
    "MAX_EPOCHS_PER_MODEL = 20 # Meant to get a decent idea of parameter, not create a final model. Behaves oddly below 3.\n",
    "HYPERBAND_ITERATIONS = 2  # \"Number of times to iterate over the full Hyperband algorithm\"\n",
    "EXECUTIONS_PER_TRIAL = 2  # Training from scratch\n",
    "SEARCH_FIT_EPOCHS = 20    # Epochs for each attempt to do a fit, I think. Not sure how this relates to MAX_EPOCHS_PER_MODEL.\n",
    "OVERWRITE = True          # I'm hoping to be able to interrupt a run and resume it later\n",
    "\n",
    "input_shape = (None, 1, CHUNK_SIZE)\n",
    "mr_t = model_tuner.ModelTuner(input_shape, OUTPUT_SIZE, CHUNK_SIZE, BATCH_SIZE)\n",
    "\n",
    "# All-encompassing optimization parameter choices. Do not try to use all of them at once...\n",
    "mr_t.CHOICES_PROCESSING_UNITS = [1, CHUNK_SIZE // 16, CHUNK_SIZE // 4, CHUNK_SIZE, CHUNK_SIZE * 2]\n",
    "mr_t.CHOICES_ACTIVATIONS = [\"elu\", \"gelu\", \"hard_sigmoid\", \"hard_silu\", \"hard_swish\", \"leaky_relu\", \"linear\", \"log_softmax\", \"mish\",\n",
    "        \"relu\", \"relu6\", \"selu\", \"sigmoid\", \"silu\", \"softmax\", \"softplus\", \"softsign\", \"swish\", \"tanh\"]\n",
    "mr_t.CHOICES_FANCY_TOPO = [\"GRU\", \"RNN\", \"LSTM\", \"GRU-RNN\", \"GRU-LSTM\", \"GRU-RNN-LSTM\"]\n",
    "mr_t.CHOICES_USE_OUTPUT_LIMITER = [True, False] # Prefers True\n",
    "mr_t.CHOICES_OPTIMIZER = [\"adamax\", \"sgd\", \"RMSProp\"]\n",
    "\n",
    "# Narrow down the choices as needed.\n",
    "mr_t.CHOICES_PROCESSING_UNITS = [1, 2, CHUNK_SIZE//16, CHUNK_SIZE//4, CHUNK_SIZE//2, CHUNK_SIZE, CHUNK_SIZE*2]\n",
    "mr_t.CHOICES_ACTIVATIONS = [\"tanh\", \"sigmoid\"]\n",
    "mr_t.CHOICES_FANCY_TOPO = [\"LSTM\"]\n",
    "mr_t.CHOICES_USE_OUTPUT_LIMITER = [True]\n",
    "mr_t.CHOICES_OPTIMIZER = [\"adamax\"]\n",
    "\n",
    "if TUNE_QUICKLY:\n",
    "    MAX_EPOCHS_PER_MODEL = 3\n",
    "    HYPERBAND_ITERATIONS = 1\n",
    "    EXECUTIONS_PER_TRIAL = 1\n",
    "    SEARCH_FIT_EPOCHS = 4\n",
    "\n",
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    return mr_t.CreateModel(hp)\n",
    "\n",
    "# Run the kerastuner search for best hyperparameters\n",
    "if TUNE_NETWORK:\n",
    "    if USE_CUSTOM_METRICS:\n",
    "        objective = kt.Objective(\"modulo_distance_accuracy\", direction=\"max\")\n",
    "    else:\n",
    "        objective = kt.Objective(f\"{MAIN_ACCURACY_METRIC}\", direction=\"max\")\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        create_model,\n",
    "        objective=objective,\n",
    "        max_epochs=MAX_EPOCHS_PER_MODEL,\n",
    "        hyperband_iterations=HYPERBAND_ITERATIONS,\n",
    "        executions_per_trial=EXECUTIONS_PER_TRIAL,\n",
    "        overwrite=OVERWRITE,\n",
    "        directory=TUNER_DIRECTORY,\n",
    "        project_name=TUNER_PROJECT_NAME)\n",
    "    tuner.search(X_train_scaled, y_train, epochs=SEARCH_FIT_EPOCHS, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "    print(f\"Best Hyper Values: {best_hyper.values}\")\n",
    "    \n",
    "    nn = tuner.get_best_models(1)[0]\n",
    "    eval_results = nn.evaluate(X_test_scaled, y_test, verbose=2, batch_size=BATCH_SIZE )\n",
    "    print(f\"Best Model Loss: {eval_results[0]}, Accuracy: {eval_results[1:]}\")\n",
    "\n",
    "    nn.save(\"./saved_models/tuned.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71ca9b-9cc3-4c85-9dab-e4b7d969ca52",
   "metadata": {},
   "source": [
    "# Model Reload /Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdee2d-2e95-4fb1-96e9-c428546102cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUILD_NETWORK:\n",
    "    print(\"Building new model\")\n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    input_shape = (None, 1, CHUNK_SIZE)\n",
    "    nn.add(tf.keras.Input(shape=input_shape[1:], name=\"Input_Layer\"))\n",
    "\n",
    "    activation_A = \"tanh\"\n",
    "    recurrent_activation_A = \"sigmoid\"\n",
    "    nn.add(tf.keras.layers.LSTM(\n",
    "        PROCESSING_UNITS, return_sequences=True, activation=activation_A, recurrent_activation=recurrent_activation_A,\n",
    "        name=f\"A_LSTM_{activation_A}_{recurrent_activation_A}\"))\n",
    "\n",
    "    nn.add(tf.keras.layers.Dense(units = OUTPUT_SIZE, activation=modulo_output, name='Modulo_Layer'))\n",
    "\n",
    "# Check the structure of the model\n",
    "print(f\"Input shape: {nn.input_shape}, Output shape: {nn.output_shape}\")\n",
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c6ff4-2ca0-4ac0-af80-7d48b157675a",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08319c4f-7080-477b-93b8-eca4791c449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the \"best\" score and set up a callback to save the model as it improves during training.\n",
    "# If you're manually training iteratively, comment this out to preserve the best-ness:\n",
    "model_checkpoint_callback = initialize_save_best(BEST_PATH)\n",
    "\n",
    "# It can be helpful to load the best, train some more, and try to improve it:\n",
    "if LOAD_BEST_MODEL:\n",
    "    if os.path.exists(BEST_PATH):\n",
    "        print(f\"Loading model from {BEST_PATH}\")\n",
    "        nn = tf.keras.models.load_model(BEST_PATH,\n",
    "            custom_objects={\n",
    "                'modulo_distance_loss': modulo_distance_loss,\n",
    "                'modulo_distance_accuracy': modulo_distance_accuracy,\n",
    "                'modulo_rounded_accuracy': modulo_rounded_accuracy,\n",
    "                'modulo_output': modulo_output\n",
    "        })\n",
    "\n",
    "# Train the model\n",
    "if TRAIN_MODEL:\n",
    "    # Decide what metrics to use\n",
    "    if USE_CUSTOM_METRICS:\n",
    "        loss = modulo_distance_loss\n",
    "        metrics = [modulo_distance_accuracy, modulo_rounded_accuracy]\n",
    "    else:\n",
    "        loss = LOSS_METRIC\n",
    "        metrics = [MAIN_ACCURACY_METRIC]\n",
    "\n",
    "    print(nn.summary())\n",
    "    print(f\"Training model\")\n",
    "    \n",
    "    if SAVE_BEST_MODEL:\n",
    "        callbacks = [model_checkpoint_callback]\n",
    "    else:\n",
    "        callbacks = None\n",
    "    \n",
    "    # Compile the Sequential model together and customize metrics\n",
    "    nn.compile(loss=loss, optimizer=OPTIMIZER, metrics=metrics)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Input shape: {nn.input_shape}, Output shape: {nn.output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253797a-b692-45c4-b2fe-89e9bb3c0f9a",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbdff5-d47a-4447-8af8-136396783596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some data for testing below...\n",
    "\n",
    "# Predicting the whole test set can take a lot of memory, so this can be used to limit it:\n",
    "TEST_SET_SIZE = X_test_scaled.shape[0]\n",
    "MAX_TEST_SUBSET = 1000\n",
    "TEST_SUBSET_SIZE = min(TEST_SET_SIZE, MAX_TEST_SUBSET) if MAX_TEST_SUBSET > 0 else TEST_SET_SIZE\n",
    "X_test_scaled_subset = X_test_scaled[0:TEST_SUBSET_SIZE, :, :]\n",
    "y_test_subset = y_test[0:TEST_SUBSET_SIZE, :, :]\n",
    "\n",
    "# Sometimes for troubleshooting I want to use the training set, which should produce more accurate predictions:\n",
    "TRAIN_SET_SIZE = X_train_scaled.shape[0]\n",
    "MAX_TRAIN_SUBSET = MAX_TEST_SUBSET\n",
    "TRAIN_SUBSET_SIZE = min(TRAIN_SET_SIZE, MAX_TRAIN_SUBSET) if MAX_TRAIN_SUBSET > 0 else TRAIN_SET_SIZE\n",
    "X_train_scaled_subset = X_train_scaled[0:TRAIN_SUBSET_SIZE, :, :]\n",
    "y_train_subset = y_train[0:TRAIN_SUBSET_SIZE, :, :]\n",
    "\n",
    "use_training_data = False\n",
    "if use_training_data:\n",
    "    print(\"Using training data as input; results are not valid for accuracy but may be informative about function\")\n",
    "    input = X_train_scaled_subset\n",
    "    expected = y_train_subset\n",
    "else:\n",
    "    print(\"Using test data as input\")\n",
    "    input = X_test_scaled_subset\n",
    "    expected = y_test_subset\n",
    "\n",
    "input.shape, expected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347990dc-230b-460f-852d-c2e0e81cef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating test set with model.evaluate() ...\")\n",
    "eval_results = nn.evaluate(X_test_scaled_subset, y_test_subset, batch_size=BATCH_SIZE)\n",
    "print(f\"Test Set        : Loss: {eval_results[0]}, Accuracy: {eval_results[1:]}\")\n",
    "\n",
    "print(\"Evaluating training set with model.evaluate() ...\")\n",
    "eval_results = nn.evaluate(X_train_scaled_subset, y_train_subset, batch_size=BATCH_SIZE)\n",
    "print(f\"Training Set    : Loss: {eval_results[0]}, Accuracy: {eval_results[1:]}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974ce11-65de-4f70-859d-558c4b6b16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was used to determine where to find the single best result from\n",
    "# the prediction. It will be moved to the analsis notebook...\n",
    "\n",
    "col_offset = []\n",
    "col_loss = []\n",
    "col_acc_dist = []\n",
    "col_acc_round = []\n",
    "\n",
    "# Looping through individual inputs is horribly slow, but I specifically wanted this\n",
    "# so I could relate behaviors to what I could actually see.\n",
    "HOW_MANY = 10\n",
    "for _ in range(HOW_MANY):\n",
    "    which_input = random.randint(0, input.shape[0]-1)\n",
    "    this_input = input[which_input:which_input+1, :, :]\n",
    "    this_expected = expected[which_input:which_input+1, :, :]\n",
    "\n",
    "    raw_predicted = nn.predict(this_input, batch_size=BATCH_SIZE)\n",
    "    this_predicted = raw_predicted.astype(np.float64)\n",
    "\n",
    "    offsets = range(0, CHUNK_SIZE)\n",
    "    #offsets = [0, CHUNK_SIZE//2, CHUNK_SIZE-1]\n",
    "\n",
    "    for offset in offsets:\n",
    "        #offset_predicted = this_predicted[:, offset, 0] #!!! Wrong for text. Can the last one just be colon?\n",
    "        offset_predicted = this_predicted[:, offset, :]\n",
    "        loss = modulo_distance_loss(this_expected, offset_predicted)\n",
    "        accuracy_distance = modulo_distance_accuracy(this_expected, offset_predicted)\n",
    "        accuracy_rounded = modulo_rounded_accuracy(this_expected, offset_predicted)\n",
    "\n",
    "        col_offset.append(offset)\n",
    "        col_loss.append(loss.numpy())\n",
    "        col_acc_dist.append(accuracy_distance.numpy())\n",
    "        col_acc_round.append(accuracy_rounded.numpy())\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Offset\": col_offset,\n",
    "    \"Loss\": col_loss,\n",
    "    \"Accuracy (Distance)\": col_acc_dist,\n",
    "    \"Accuracy (Rounded)\": col_acc_round \n",
    "}).set_index(\"Offset\")\n",
    "\n",
    "metrics_df = metrics_df.groupby(['Offset']).mean()\n",
    "print(metrics_df.describe())\n",
    "metrics_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488ae3d-b10f-40d7-826a-61fedba4cb2b",
   "metadata": {},
   "source": [
    "# Model Usefulness Spot-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd4fd4-831e-43a5-9351-c0c2cfa1be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caesar_Cracker(object):\n",
    "    def __init__(self, scaler, key_model, text_model):\n",
    "        self.scaler = scaler\n",
    "        self.key_model = key_model\n",
    "        self.text_model = text_model\n",
    "\n",
    "    def infer_text_with_model(self, ciphertext: str) -> str:\n",
    "        chunk_size = self.text_model.input_shape[2]\n",
    "        offsets = encoders.string_to_offsets(ciphertext)\n",
    "        chunks = helpers.chunkify(offsets, chunk_size)\n",
    "        scaled_chunks = self.scaler.transform(chunks)\n",
    "        shaped_chunks = tf_helpers.reshape_input(np.array(scaled_chunks), chunk_size)\n",
    "        guesses = self.text_model.predict(shaped_chunks, verbose=0)\n",
    "        # Shape of prediction:\n",
    "        # (chunk count, chunk size, chunk size)\n",
    "        # ... so that's a little confusing. For keys, the 2nd dimension corresponds to time,\n",
    "        # so get the final (best?) guesses\n",
    "        best_guesses = guesses[:, chunk_size-1, :]\n",
    "\n",
    "        # Now we have floating point offsets. We want integer offsets, then strings:\n",
    "        flat = best_guesses.flatten()        \n",
    "        int_offsets = flat.round().astype(int)\n",
    "        result = encoders.offsets_to_string(int_offsets)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def infer_key_with_model(self, ciphertext: str) -> int:\n",
    "        chunk_size = self.key_model.input_shape[2]\n",
    "        offsets = encoders.string_to_offsets(ciphertext)\n",
    "        chunks = helpers.chunkify(offsets, chunk_size)\n",
    "        scaled_chunks = self.scaler.transform(chunks)\n",
    "        shaped_chunks = tf_helpers.reshape_input(np.array(scaled_chunks), chunk_size)\n",
    "        keys = self.key_model.predict(shaped_chunks, verbose=0)\n",
    "        # Shape of keys:\n",
    "        # (chunk count, chunk size, 1)\n",
    "\n",
    "        # The model puts out a key for every iteration through the data, and (on average)\n",
    "        # gets more accurate every time, so the best key is the last one:\n",
    "        best_keys = keys[:, chunk_size-1, :]\n",
    "\n",
    "        # We have the best key from each chunk, so pick the middle one:\n",
    "        key = np.median(best_keys)\n",
    "        return int(round(key))\n",
    "\n",
    "FILES_TO_CHECK = 30\n",
    "if INFER_TEXT:    \n",
    "    CHUNKS_TO_CHECK = 1\n",
    "else:\n",
    "    CHUNKS_TO_CHECK = 200\n",
    "\n",
    "good=0\n",
    "bad=0\n",
    "for _ in range(FILES_TO_CHECK):\n",
    "    sid = random.choice(list(sid_to_c.keys()))\n",
    "    cipher_file_db = random.choice(sid_to_c[sid])\n",
    "    ciphertext_path = cipher_file_db.path\n",
    "    ciphertext = helpers.read_text_file(ciphertext_path)\n",
    "    length = min(CHUNK_SIZE * CHUNKS_TO_CHECK, len(ciphertext))\n",
    "    ciphertext = ciphertext[0:length]\n",
    "    \n",
    "    if INFER_KEY:\n",
    "        cracker = Caesar_Cracker(X_scaler, nn, None)\n",
    "    \n",
    "        with db.get_session() as session:\n",
    "            correct_key = int(db.get_key_by_id(session, cipher_file_db.key_id).value)\n",
    "        \n",
    "        inferred_key = cracker.infer_key_with_model(ciphertext)\n",
    "\n",
    "        if correct_key == inferred_key:\n",
    "            good += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "    if INFER_TEXT:\n",
    "        cracker = Caesar_Cracker(X_scaler, None, nn)\n",
    "    \n",
    "        with db.get_session() as session:\n",
    "            correct_key = int(db.get_key_by_id(session, cipher_file_db.key_id).value)\n",
    "            \n",
    "        inferred_text = cracker.infer_text_with_model(ciphertext)\n",
    "        #!!! print(ciphertext[0:128])\n",
    "        #!!! print(inferred_text[0:128])\n",
    "        #!!! print()\n",
    "\n",
    "        if True:\n",
    "            # Checking distribution\n",
    "            offsets = encoders.string_to_offsets(ciphertext)\n",
    "            chunks = helpers.chunkify(offsets, cracker.text_model.chunk_size)\n",
    "            scaled_chunks = cracker.scaler.transform(chunks)\n",
    "            shaped_chunks = tf_helpers.reshape_input(np.array(scaled_chunks), cracker.text_model.chunk_size)\n",
    "            guesses = cracker.text_model.predict(shaped_chunks, verbose=0)\n",
    "            best_guesses = guesses[:, cracker.text_model.chunk_size-1, :]\n",
    "            flat = best_guesses.flatten()        \n",
    "            int_offsets = flat.round().astype(int)\n",
    "\n",
    "            print(pd.Series(int_offsets).value_counts())\n",
    "\n",
    "if bad > 0:\n",
    "    print(good, bad, float(good)/float(good+bad))\n",
    "    print(f\"Caesar chance would be {float(1)/float(len(encoders.CHARSET))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
