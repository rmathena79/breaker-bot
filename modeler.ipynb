{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bca2471e-572c-4233-b18c-e8e03e8cf40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "from credentials import SAMPLE_DB, FULL_DB\n",
    "from librarian import SAMPLE_DATA_DIR\n",
    "\n",
    "import encoders\n",
    "import db_connect\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9bdcd65-ceca-4b1a-921a-4544e38b37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = SAMPLE_DATA_DIR\n",
    "CHUNK_SIZE = 128\n",
    "LAYER_UNITS = max(1, CHUNK_SIZE // 10)\n",
    "\n",
    "INFER_TEXT = True\n",
    "INFER_KEY = not INFER_TEXT\n",
    "\n",
    "if INFER_TEXT:\n",
    "    MAIN_ACCURACY_METRIC = \"mae\"\n",
    "    GOOD_ACCURACY_DIRECTION = \"min\" # Some accuracy metrics go up for better results, some go down\n",
    "    LOSS_METRIC = \"mean_squared_error\"\n",
    "    OUTPUT_SIZE = CHUNK_SIZE\n",
    "    OPTIMIZER = \"sgd\"\n",
    "else:\n",
    "    MAIN_ACCURACY_METRIC = \"mae\"\n",
    "    GOOD_ACCURACY_DIRECTION = \"min\" # Some accuracy metrics go up for better results, some go down\n",
    "    LOSS_METRIC = \"mae\"\n",
    "    OUTPUT_SIZE = 1 # actually depends on cipher\n",
    "    OPTIMIZER = \"adamax\"\n",
    "\n",
    "ENCRYPTED_FILE_LIMIT = 10 # -1 to disable limit\n",
    "\n",
    "BASE_TRAIN_PCT = 0.75   # Start here. If it exceed the max count, reduce it. Note 0.75 is the default.\n",
    "MAX_TRAIN_COUNT = 100000 # -1 to disable\n",
    "SPLIT_SEED = 42\n",
    "\n",
    "LOAD_BEST_MODEL = False # If False, a new model will be created from scratch\n",
    "SAVE_BEST_MODEL = False\n",
    "TRAIN_MODEL = True\n",
    "EPOCHS = 5\n",
    "\n",
    "db = db_connect.DB(SAMPLE_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "789ab2e5-5e11-4812-91be-bb9a63acb6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder IDs: {'None': 1, 'Simplifier': 2, 'Caesar Cipher': 3, 'Substitution Cipher': 4, 'Enigma Machine': 5}\n",
      "Key Type IDs: {'Character Offset': 1, 'Character Map': 2, 'Rotor Settings': 3}\n"
     ]
    }
   ],
   "source": [
    "# Get database IDs for encoders and key types\n",
    "\n",
    "encoder_ids= {}\n",
    "key_type_ids = {}\n",
    "\n",
    "with db.get_session() as session:\n",
    "    for encoder in encoders.ALL_ENCODER_NAMES:\n",
    "        id = db.get_encoder_id(session, encoder)\n",
    "        encoder_ids[encoder] = id\n",
    "\n",
    "    print(f\"Encoder IDs: {encoder_ids}\")\n",
    "\n",
    "    for key_type in encoders.KEY_NAMES:\n",
    "        id = db.get_key_type_id(session, key_type)\n",
    "        key_type_ids[key_type] = id\n",
    "\n",
    "    print(f\"Key Type IDs: {key_type_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fc8e5ca-4b37-4947-9e79-f061cc993a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map source ID to plaintext file (1) details, and source ID to corresponding ciphertext files (1+) details\n",
    "sid_to_p = {}\n",
    "sid_to_c = {}\n",
    "\n",
    "cipher_id = encoder_ids[encoders.ENCODER_CAESAR]\n",
    "with db.get_session() as session:\n",
    "    # Get all files encrypted with the cipher we care about\n",
    "    encrypted_files = db.get_files_by_source_and_encoder(session, -1, cipher_id)\n",
    "\n",
    "    if len(encrypted_files) > ENCRYPTED_FILE_LIMIT and ENCRYPTED_FILE_LIMIT > 0:\n",
    "        encrypted_files = random.sample(encrypted_files, ENCRYPTED_FILE_LIMIT)\n",
    "\n",
    "    for c in encrypted_files:\n",
    "        sid = c.source_id\n",
    "    \n",
    "        if sid not in sid_to_p:\n",
    "            plaintext_ids = db.get_files_by_source_and_encoder(session, sid, encoder_ids[encoders.ENCODER_SIMPLIFIER])\n",
    "            if len(plaintext_ids) != 1:\n",
    "                raise Exception(f\"Found {len(plaintext_ids)} plaintexts for source ID {sid}; should be exactly 1\")\n",
    "            sid_to_p[sid] = plaintext_ids[0]\n",
    "\n",
    "        if sid not in sid_to_c:\n",
    "            sid_to_c[sid] = []\n",
    "        sid_to_c[sid].append(c)\n",
    "\n",
    "len(sid_to_p), len(sid_to_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3f6de1c-79f8-4637-8dec-ade266665517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34472, 128),\n",
       " (34472, 128),\n",
       " (128,),\n",
       " (128,),\n",
       " array([58., 58., 71., 73., 70., 47., 76., 46., 60., 47., 39., 44., 80.,\n",
       "        39., 47., 60., 34., 76., 73., 10., 47., 60., 73., 40., 39., 44.,\n",
       "        73., 10., 34., 69., 39., 78., 10., 67., 46., 70., 79., 39., 34.,\n",
       "        69., 47., 39., 75., 32., 60., 39., 70., 69., 67., 10., 69., 60.,\n",
       "        58., 47., 10., 74., 75., 73., 10., 44., 76., 75., 60., 47., 39.,\n",
       "        71., 73., 70., 70., 62., 73., 60., 34., 47., 10., 69., 63., 39.,\n",
       "        75., 60., 34., 68., 39., 34., 75., 39., 32., 75., 75., 71., 38.,\n",
       "        43., 43., 78., 78., 78., 41., 71., 63., 47., 71., 41., 69., 60.,\n",
       "        75., 39., 45., 75., 32., 10., 74., 58., 62., 10., 67., 60., 39.,\n",
       "        78., 34., 74., 39., 71., 73., 70., 47., 76., 46., 60.]),\n",
       " array([10., 10., 80., 82., 79., 68., 85., 67., 69., 68., 32., 66., 89.,\n",
       "        32., 68., 69., 65., 85., 82., 73., 68., 69., 82., 44., 32., 66.,\n",
       "        82., 73., 65., 78., 32., 87., 73., 76., 67., 79., 88., 32., 65.,\n",
       "        78., 68., 32., 84., 72., 69., 32., 79., 78., 76., 73., 78., 69.,\n",
       "        10., 68., 73., 83., 84., 82., 73., 66., 85., 84., 69., 68., 32.,\n",
       "        80., 82., 79., 79., 70., 82., 69., 65., 68., 73., 78., 71., 32.,\n",
       "        84., 69., 65., 77., 32., 65., 84., 32., 72., 84., 84., 80., 58.,\n",
       "        47., 47., 87., 87., 87., 46., 80., 71., 68., 80., 46., 78., 69.,\n",
       "        84., 32., 40., 84., 72., 73., 83., 10., 70., 73., 76., 69., 32.,\n",
       "        87., 65., 83., 32., 80., 82., 79., 68., 85., 67., 69.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build up the features (X, the cipher texts as values) and targets (y, either the plain texts as values OR the key).\n",
    "# Note targets are not necessarily unique.\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with db.get_session() as session:\n",
    "    for sid in sid_to_p:\n",
    "        if INFER_TEXT:\n",
    "            plaintext = helpers.read_text_file(sid_to_p[sid].path)\n",
    "            target_chunks = helpers.string_to_bytes(plaintext, CHUNK_SIZE)    \n",
    "    \n",
    "        for c in sid_to_c[sid]:\n",
    "            ciphertext = helpers.read_text_file(c.path)\n",
    "            feature_chunks = helpers.string_to_bytes(ciphertext, CHUNK_SIZE)\n",
    "\n",
    "            if INFER_KEY:\n",
    "                key_value = float(db.get_key_by_id(session, c.key_id).value)\n",
    "    \n",
    "            for i in range (len(feature_chunks)):\n",
    "                X.append(feature_chunks[i])\n",
    "\n",
    "                if INFER_TEXT:\n",
    "                    y.append(target_chunks[i])                \n",
    "\n",
    "                if INFER_KEY:\n",
    "                    y.append(key_value)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X.shape, y.shape, X[0].shape, y[0].shape, X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e55451fa-ee72-4835-b18a-d03d44899412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count would be 25854\n",
      "Train count is 25854\n",
      "34472 34472 25854 8618 25854 8618\n"
     ]
    }
   ],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "train_count = int(round(len(y) * BASE_TRAIN_PCT))\n",
    "print(f\"Train count would be {train_count}\")\n",
    "if train_count > MAX_TRAIN_COUNT and MAX_TRAIN_COUNT > -1:\n",
    "    train_count = int(MAX_TRAIN_COUNT)\n",
    "print(f\"Train count is {train_count}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_count, random_state=SPLIT_SEED)\n",
    "print( len(X), len(y), len(X_train), len(X_test), len(y_train), len(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d5fd198-2be6-4f66-91ff-0bbb1ccb819f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25854, 128), (8618, 128))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "X_train_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0fdee2d-2e95-4fb1-96e9-c428546102cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if LOAD_BEST_MODEL:\n",
    "    print(f\"Loading model from {BEST_PATH}\")\n",
    "    nn = tf.keras.models.load_model(BEST_PATH)\n",
    "else:\n",
    "    print(\"Building new model\")\n",
    "    in_shape = (CHUNK_SIZE,)\n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    try_RNN = True\n",
    "    if try_RNN:\n",
    "        nn.add(tf.keras.layers.Embedding(input_dim=CHUNK_SIZE, output_dim=OUTPUT_SIZE))\n",
    "        nn.add(tf.keras.layers.LSTM(CHUNK_SIZE))\n",
    "        nn.add(tf.keras.layers.Dense(units=CHUNK_SIZE, activation=\"relu\"))\n",
    "        nn.add(tf.keras.layers.Dense(OUTPUT_SIZE))\n",
    "\n",
    "    if not try_RNN:\n",
    "        # Input layer\n",
    "        nn.add(tf.keras.Input(shape=in_shape))\n",
    "        \n",
    "        # Hidden layers\n",
    "        activations = [\"tanh\", \"relu\", \"elu\", \"exponential\", \"gelu\", \"mish\", \"relu6\", \"tanh\", \"selu\"]\n",
    "        unit_counts = [LAYER_UNITS]\n",
    "        for u in unit_counts:\n",
    "            for a in activations:\n",
    "                nn.add(tf.keras.layers.Dense(units=u, activation=a))\n",
    "        \n",
    "        # Output layer\n",
    "        nn.add(tf.keras.layers.Dense(units=OUTPUT_SIZE))\n",
    "    \n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08319c4f-7080-477b-93b8-eca4791c449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Epoch 1/5\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 954.7554 - mae: 23.9296\n",
      "Epoch 2/5\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 413.4129 - mae: 16.0348\n",
      "Epoch 3/5\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 403.2758 - mae: 15.7707\n",
      "Epoch 4/5\n",
      "\u001b[1m797/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 396.4476 - mae: 15.6169"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set up training checkpoint to save after each epoch, if it is a new best model:\n",
    "BEST_PATH = './best.keras'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=BEST_PATH,\n",
    "    monitor=MAIN_ACCURACY_METRIC,\n",
    "    mode=GOOD_ACCURACY_DIRECTION,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    print(f\"Training model\")\n",
    "    \n",
    "    # Compile the Sequential model together and customize metrics\n",
    "    nn.compile(loss=LOSS_METRIC, optimizer=OPTIMIZER, metrics=[MAIN_ACCURACY_METRIC])\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    callbacks = None\n",
    "    if SAVE_BEST_MODEL:\n",
    "        callbacks = [model_checkpoint_callback]\n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=EPOCHS, callbacks=callbacks)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4abd4fd4-831e-43a5-9351-c0c2cfa1be2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Decoded   :  CBBBBBACACCBACACBBBBABBCCCABBBCBBBBCACACBCDCABCBBCABBBBDCABCCACCBAADBBACABCCBADBBCBBCBACCABACCACBCBBBDBBCBBCD@CBCBCBACDBBDDCAACCCBBBBBACACCBACACBBBBABBCCCABBBCBBBBCACACBCDCABCBBCABBBBDCABCCACCBAADBBACABCCBADBBCBBCBACCABACCACBCBBBDBBCBBCD@CBCBCBACDBBDDCAACC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,968,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m16,384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)            │     \u001b[38;5;34m1,968,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m82,048\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,067,074</span> (7.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,067,074\u001b[0m (7.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,067,072</span> (7.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,067,072\u001b[0m (7.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_chunks_with_model(chunks: list[list], model, scaler, input_already_scaled = True) -> list[list]:\n",
    "    if input_already_scaled:\n",
    "        return model.predict(chunks)\n",
    "    else:\n",
    "        return model.predict(scaler.transform(chunks))\n",
    "\n",
    "def decode_text_with_model(ciphertext: str, model, scaler) -> str:\n",
    "    chunks = helpers.string_to_bytes(ciphertext, CHUNK_SIZE)\n",
    "    decoded_chunks = decode_chunks_with_model(chunks, model, scaler, input_already_scaled = False)\n",
    "    return helpers.bytes_to_string(decoded_chunks)\n",
    "\n",
    "def infer_key_with_model(ciphertext: str, model, scaler) -> int:\n",
    "    chunks = helpers.string_to_bytes(ciphertext, CHUNK_SIZE)\n",
    "    key = int(round(model.predict(scaler.transform(chunks))[0][0]))\n",
    "    return key\n",
    "\n",
    "cipher_file_db = sid_to_c[list(sid_to_c.keys())[0]][0]\n",
    "ciphertext_path = cipher_file_db.path\n",
    "ciphertext = helpers.read_text_file(ciphertext_path)\n",
    "ciphertext = ciphertext[0:CHUNK_SIZE*2]\n",
    "    \n",
    "if INFER_TEXT:    \n",
    "    print(\"Decoded   : \", decode_text_with_model(ciphertext, nn, X_scaler))\n",
    "if INFER_KEY:\n",
    "    with db.get_session() as session:\n",
    "        correct_key = int(db.get_key_by_id(session, cipher_file_db.key_id).value)\n",
    "    print(\"Correct Key: \", correct_key)\n",
    "    \n",
    "    inferred_key = infer_key_with_model(ciphertext, nn, X_scaler)\n",
    "    print(\"Inferred Key: \", inferred_key)\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1c384-8e11-4541-a33c-59f14cb7ecf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
