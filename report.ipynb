{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090d4fc1-8fc8-418e-99ed-e358c76882df",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f20f08-991d-402e-8573-bff019069173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 22:22:23.077086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-02 22:22:23.095211: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-02 22:22:23.099831: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Disable some chatty warnings from Tensorflow:\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
    "os.environ[\"GLOG_minloglevel\"] = \"2\"\n",
    "\n",
    "# Still trying to disable warnings. It's harder than you'd think!\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from credentials import CONNECTION_INFO\n",
    "from constants import *\n",
    "\n",
    "import encoders\n",
    "import db_connect\n",
    "import helpers\n",
    "import tf_helpers\n",
    "import models\n",
    "import crackers\n",
    "\n",
    "# Callbacks for use with TensorFlow\n",
    "from tf_helpers import modulo_output, modulo_distance_loss, modulo_distance_accuracy, modulo_rounded_accuracy, initialize_save_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fe4879-663f-48a5-8d5b-4848e89f7f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733206946.585698  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733206946.712009  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733206946.712071  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733206946.716255  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733206946.716319  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733206946.716340  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733206948.109496  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733206948.109671  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733206948.109929  175932 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "ENCODER = encoders.ENCODER_CAESAR\n",
    "\n",
    "CAESAR_KEY_MODEL = models.load_model(models.CAESAR_KEY_MODEL_PATH)\n",
    "CAESAR_TEXT_MODEL = models.load_model(models.CAESAR_TEXT_MODEL_PATH)\n",
    "CAESAR_CHUNK_SIZE = CAESAR_TEXT_MODEL.input_shape[2]\n",
    "CAESAR_KEY_SIZE = 1\n",
    "CAESAR_SCALER = helpers.load_scaler_from_file(helpers.get_recommended_scaler_path(encoders.ENCODER_CAESAR, CAESAR_CHUNK_SIZE, temp=False))\n",
    "\n",
    "CAESAR_CRACKER = crackers.Caesar_Cracker(CAESAR_SCALER, CAESAR_KEY_MODEL, CAESAR_TEXT_MODEL, verbose=0)\n",
    "\n",
    "KEY_BATCH_SIZE = 256\n",
    "TEXT_BATCH_SIZE = 32\n",
    "\n",
    "# You may need to limit test size for memory reasons, espescially for the text model.\n",
    "# Set to -1 to disable the limit.\n",
    "KEY_INPUT_CHUNK_LIMIT = -1\n",
    "TEXT_INPUT_CHUNK_LIMIT = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1c379-1d96-4848-9c81-7bcc8418f97f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb0bb7d-d7a4-4522-a2ee-63d91e325b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 original sources and 8 encrypted files\n",
      "There are 41410 chunks of ciphertext for the key model, and 10000 for the text model.\n",
      "Shapes:  (41410, 256) (10000, 256) (41410,) (10000, 256)\n"
     ]
    }
   ],
   "source": [
    "# Get some test data. This all comes from texts that have not been used for training at all.\n",
    "db = db_connect.DB(CONNECTION_INFO)\n",
    "\n",
    "with db.get_session() as session:\n",
    "    # Get database IDs for encoders and key types\n",
    "    (encoder_ids, key_type_id) = db.get_id_maps(session)\n",
    "\n",
    "    # Map source ID to plaintext file (1) details, and source ID to corresponding ciphertext files (1+) details.\n",
    "    # Only files from a \"test only\" source will be returned.\n",
    "    (sid_to_p, sid_to_c) = db.get_source_maps(session, -1, encoder_ids[ENCODER], test_only=True)\n",
    "\n",
    "    # Get the features (X, the cipher texts as offsets) and targets (y, either the plain texts as offsets OR the key).\n",
    "    (X, y_keys, y_texts) = db.get_features_and_targets(\n",
    "            session, sid_to_p, sid_to_c, ENCODER, CAESAR_CHUNK_SIZE, \n",
    "            want_keys=True, want_texts=True)\n",
    "\n",
    "# We may want to limit how many we test with, for memory reasons\n",
    "key_feature_limit = min(len(X), KEY_INPUT_CHUNK_LIMIT) if KEY_INPUT_CHUNK_LIMIT > -1 else len(X)\n",
    "text_feature_limit = min(len(X), TEXT_INPUT_CHUNK_LIMIT) if TEXT_INPUT_CHUNK_LIMIT > -1 else len(X)\n",
    "\n",
    "# Put into Numpy arrays for easier handling\n",
    "X_for_keys = np.array(X[0:key_feature_limit])\n",
    "X_for_texts = np.array(X[0:text_feature_limit])\n",
    "y_keys = np.array(y_keys[0:key_feature_limit])\n",
    "y_texts = np.array(y_texts[0:text_feature_limit])\n",
    "\n",
    "del X\n",
    "\n",
    "print(f\"There are {len(sid_to_p)} original sources and {len(sid_to_c)} encrypted files\")\n",
    "print(f\"There are {X_for_keys.shape[0]} chunks of ciphertext for the key model, and {X_for_texts.shape[0]} for the text model.\")\n",
    "print(\"Shapes: \", X_for_keys.shape, X_for_texts.shape, y_keys.shape, y_texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889406f4-84d0-4f92-9d67-5adfdff4a256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41410, 256, 1), (10000, 256, 1), (41410, 1, 1), (10000, 256, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale and shape the data\n",
    "X_for_keys_scaled = CAESAR_SCALER.transform(X_for_keys)\n",
    "X_for_texts_scaled = CAESAR_SCALER.transform(X_for_texts)\n",
    "\n",
    "X_for_keys_scaled = tf_helpers.reshape_input_for_RNN(X_for_keys_scaled, CAESAR_CHUNK_SIZE)\n",
    "X_for_texts_scaled = tf_helpers.reshape_input_for_RNN(X_for_texts_scaled, CAESAR_CHUNK_SIZE)\n",
    "y_keys = tf_helpers.reshape_output_for_RNN(y_keys, CAESAR_KEY_SIZE)\n",
    "y_texts = tf_helpers.reshape_output_for_RNN(y_texts, CAESAR_CHUNK_SIZE)\n",
    "\n",
    "X_for_keys_scaled.shape, X_for_texts_scaled.shape, y_keys.shape, y_texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3ceb5-31d3-4567-9a1d-5def5525315d",
   "metadata": {},
   "source": [
    "# Basic Functionality and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31682a2-e588-4162-a609-82be22d648a8",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "To make things cleaner, I simplified the texts to all uppercase and removed most special characters.\n",
    "\n",
    "Tensorflow models work on numbers, not letters, so I needed a way to represent the input numerically. At first I just used ASCII codes, but they are nonconsequential and don't start at 0, so that made the problem harder than necessary. I settled on representing each character as its index in my simplified character set. Note numbers, punctuation, and whitespace characters are included; traditionally those would be left alone, providing big clues to whoever wanted to crack the code.\n",
    "\n",
    "Because Tensorflow models need a fixed input size, the text is broken into \"chunks\". The current process actually causes some duplication near the end of the input.\n",
    "\n",
    "Finally, everything needs to be put into lists or arrays for efficient handling. The shape of the data, espescially as output by the model, was actually a major stumbling point for me. I still don't really get what the model is doing. I just think I know where to look for good numbers.\n",
    "\n",
    "This is all demonstrated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c7655b8-2915-403f-bcbc-676f52784525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text      : Resumé.       spaces.\n",
      "Simplified text   : RESUME. SPACES.\n",
      "Encoded as offsets: [17, 4, 18, 20, 12, 4, 55, 60, 18, 15, 0, 2, 4, 18, 55]\n",
      "Chunked           : [[17, 4, 18, 20], [12, 4, 55, 60], [18, 15, 0, 2], [2, 4, 18, 55]]\n",
      "Reconstructed     : RESUME. SPACCES. -- note it doesn't match perfectly, which is a known issue\n",
      "\n",
      "Shapes of the data we're actually using in this notebook:\n",
      "X / features (chunks of ciphertext): (41410, 256)\n",
      "y / targets for the key model      : (41410, 1, 1)\n",
      "y / targets for the text model     : (10000, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "initial_text = \"Resumé.       spaces.\"\n",
    "simplified = encoders.encode_simple(initial_text)\n",
    "offsets = encoders.string_to_offsets(simplified)\n",
    "chunks = helpers.chunkify(offsets, 4)\n",
    "\n",
    "# Simplification cannot be reverse, but everything else can be:\n",
    "unchunked = np.array(chunks).flatten()\n",
    "reconstructed = encoders.offsets_to_string(unchunked)\n",
    "\n",
    "print(f\"Initial text      : {initial_text}\")\n",
    "print(f\"Simplified text   : {simplified}\")\n",
    "print(f\"Encoded as offsets: {offsets}\")\n",
    "print(f\"Chunked           : {chunks}\")\n",
    "print(f\"Reconstructed     : {reconstructed} -- note it doesn't match perfectly, which is a known issue\")\n",
    "print()\n",
    "print(\"Shapes of the data we're actually using in this notebook:\")\n",
    "print(f\"X / features (chunks of ciphertext): {X_for_keys.shape}\")\n",
    "print(f\"y / targets for the key model      : {y_keys.shape}\")\n",
    "print(f\"y / targets for the text model     : {y_texts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfd749-66c2-4871-ab37-dec29cfe34aa",
   "metadata": {},
   "source": [
    "## Metrics as Gathered by Tensorflow\n",
    "\n",
    "Tensorflow provides some basic accuracy functions, but I had to add some custom ones. The main problem is that the cipher algorithms treat values as essentially circular. If there are 60 possible characters, ranged 0-59, then numbers 2 and 58 are only 4 away from each other. Tensorflow's metrics would treat them as 56 away from each other, leading to at best misleading results.\n",
    "\n",
    "In other words, it needs to do modular arithmetic.\n",
    "\n",
    "The are the custom metrics I've used:\n",
    "* **modulo_distance_loss (MDL)**: My loss function, returning the distance between true and predicted values.\n",
    "* **modulo_distance_accuracy (MDA)**: My main accuracy function, very similar to loss, but with distance expressed as percentage away from maximum. So it goes up as the results get better!\n",
    "* **modulo_rounded_accuracy (MRA)**: A more intuitive, but less mathematically helpful, accuracy calculation. It rounds the values (as would be done for final use of the output) and returns the percent of predictions which exactly match true values.\n",
    "\n",
    "I suspect I am not consistently using metric functions correctly. Functions like Model.evaluate() may be returning misleading values. This is illustrated by the fact that my loss function returns slightly different results when used as a loss function, as opposed to a metric, in the same run.\n",
    "\n",
    "The next cell gathers these basic metrics, as reported by Model.evaluate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c857e34f-b7b0-4cb6-8879-d261d216a98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Key Model</th>\n",
       "      <th>Text Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MDL as Loss</td>\n",
       "      <td>0.601246</td>\n",
       "      <td>1.123931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDL</td>\n",
       "      <td>0.600890</td>\n",
       "      <td>1.124421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDA</td>\n",
       "      <td>0.980617</td>\n",
       "      <td>0.963728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MRA</td>\n",
       "      <td>0.626832</td>\n",
       "      <td>0.452254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(TF) MSE</td>\n",
       "      <td>91.183128</td>\n",
       "      <td>172.364807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(TF) Accuracy</td>\n",
       "      <td>0.041742</td>\n",
       "      <td>0.003678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Metric  Key Model  Text Model\n",
       "0    MDL as Loss   0.601246    1.123931\n",
       "1            MDL   0.600890    1.124421\n",
       "2            MDA   0.980617    0.963728\n",
       "3            MRA   0.626832    0.452254\n",
       "4       (TF) MSE  91.183128  172.364807\n",
       "5  (TF) Accuracy   0.041742    0.003678"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Model.evaluate() to gather some basic metrics across the whole test set\n",
    "\n",
    "# Associate some interesting metrics with the models:\n",
    "#\n",
    "# From Tensorflow:\n",
    "# Mean Squared Error (MSE) is often good for a loss function.\n",
    "# Accuracy is meant mainly for classification, but might be somewhat relevant\n",
    "#\n",
    "# My functions are described above.\n",
    "loss_metric = modulo_distance_loss\n",
    "all_metrics = [modulo_distance_loss, modulo_distance_accuracy, modulo_rounded_accuracy, \"mse\", \"accuracy\"]\n",
    "\n",
    "# Names for the dataframe, must line up with evaluate() output\n",
    "metric_names = [\"MDL as Loss\", \"MDL\", \"MDA\", \"MRA\", \"(TF) MSE\", \"(TF) Accuracy\"]\n",
    "\n",
    "CAESAR_KEY_MODEL.compile(loss=loss_metric, optimizer=PREFERRED_OPTIMIZER, metrics=all_metrics)\n",
    "CAESAR_TEXT_MODEL.compile(loss=loss_metric, optimizer=PREFERRED_OPTIMIZER, metrics=all_metrics)\n",
    "\n",
    "key_results = CAESAR_KEY_MODEL.evaluate(X_for_keys_scaled, y_keys, batch_size=KEY_BATCH_SIZE, verbose=0)\n",
    "text_results = CAESAR_TEXT_MODEL.evaluate(X_for_texts_scaled, y_texts, batch_size=TEXT_BATCH_SIZE, verbose=0)\n",
    "\n",
    "evaluate_df = pd.DataFrame({\n",
    "    \"Metric\": metric_names,\n",
    "    \"Key Model\": key_results, \n",
    "    \"Text Model\": text_results })\n",
    "    \n",
    "\n",
    "# Put my preferred metrics back in for later testing\n",
    "loss_metric = modulo_distance_loss\n",
    "all_metrics = [modulo_distance_accuracy, modulo_rounded_accuracy]\n",
    "CAESAR_KEY_MODEL.compile(loss=loss_metric, optimizer=PREFERRED_OPTIMIZER, metrics=all_metrics)\n",
    "CAESAR_TEXT_MODEL.compile(loss=loss_metric, optimizer=PREFERRED_OPTIMIZER, metrics=all_metrics)\n",
    "\n",
    "evaluate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bdefbd-3a7c-4319-9c91-1c8d361331f4",
   "metadata": {},
   "source": [
    "## Comparison to Direct Accuracy Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779fc4cf-8d3e-4979-afec-5f9d764e8b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41410, 256, 1), (10000, 256, 256))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call predict() directly, extract the values we'd actually want to use, and calculate accuracy in a nice simple way\n",
    "key_predictions = CAESAR_KEY_MODEL.predict(X_for_keys_scaled, verbose=0, batch_size=KEY_BATCH_SIZE)\n",
    "text_predictions = CAESAR_TEXT_MODEL.predict(X_for_texts_scaled, verbose=0, batch_size=TEXT_BATCH_SIZE)\n",
    "\n",
    "key_predictions.shape, text_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fef634-e2e6-4890-ac6a-ed31361884cb",
   "metadata": {},
   "source": [
    "## Output Data Structure\n",
    "The model outputs predictions in a particular shape, which apparently corresponds to the prediction getting refined as the LSTM layer moves through the input. So instead of one prediction, you get 256 (the chunk size I've been using).\n",
    "\n",
    "Both models produce 3-dimensional output, but the key model is only size 1 along the final axis. So it's easier to work with. The shape is:\n",
    "    (feature index, chunk size, 1)\n",
    "\n",
    "The final key predictions are at the end of the 2nd dimension.\n",
    "\n",
    "The text model output has this shape:\n",
    "    (feature index, chunk size, chunk size)\n",
    "... and for some reason, the final predictions seem to be at the end of 3rd dimension, not the 2nd. I do not know why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99dc1193-daaa-4e88-b9bf-e0b5e0547121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41410, 1), (10000, 256), (41410, 1, 1), (10000, 256, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_keys = key_predictions[:, CAESAR_CHUNK_SIZE-1, :]\n",
    "best_texts = text_predictions[:, :, CAESAR_CHUNK_SIZE-1]\n",
    "\n",
    "best_keys.shape, best_texts.shape, y_keys.shape, y_texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435f5f7-83e9-4059-9e58-d8f483e713f0",
   "metadata": {},
   "source": [
    "## Direct Accuracy Calculations\n",
    "This cell calculates accuracy in a much more straightforward, understandable, transparent way. It just gets the value that would actually be used from the prediction, and counts how many of the predictions are exactly right.\n",
    "\n",
    "This shows that my custom metric functions are in the right ballpark, especially MRA, when used with evaluate(). But they're quite lining up with what one would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef5ff72c-7cdc-4519-80b2-41985db8f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A true string:\n",
      "\n",
      "\n",
      "PRODUCED BY GREG WEEKS, MARY MEEHAN AND THE ONLINE\n",
      "DISTRIBUTED...\n",
      "\n",
      "A predicted string:\n",
      "HFTSPDSBDD CT\n",
      "FREG UDDJT??MAQT\n",
      "LDDGAM AND UHE POMJOFAEJUTJIBUTDD...\n",
      "\n",
      "The key model produced the right key 71.38% of the time.\n",
      "The text model got 45.30% of the characters right.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Key Model</th>\n",
       "      <th>Text Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MDL as Loss</td>\n",
       "      <td>0.601246</td>\n",
       "      <td>1.123931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDL</td>\n",
       "      <td>0.600890</td>\n",
       "      <td>1.124421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDA</td>\n",
       "      <td>0.980617</td>\n",
       "      <td>0.963728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MRA</td>\n",
       "      <td>0.626832</td>\n",
       "      <td>0.452254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(TF) MSE</td>\n",
       "      <td>91.183128</td>\n",
       "      <td>172.364807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(TF) Accuracy</td>\n",
       "      <td>0.041742</td>\n",
       "      <td>0.003678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Direct Accuracy</td>\n",
       "      <td>0.713765</td>\n",
       "      <td>0.453007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Metric  Key Model  Text Model\n",
       "0      MDL as Loss   0.601246    1.123931\n",
       "1              MDL   0.600890    1.124421\n",
       "2              MDA   0.980617    0.963728\n",
       "3              MRA   0.626832    0.452254\n",
       "4         (TF) MSE  91.183128  172.364807\n",
       "5    (TF) Accuracy   0.041742    0.003678\n",
       "6  Direct Accuracy   0.713765    0.453007"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The predictions come out as floating point values, so round and convert to integer. Not rounding caused me a lot of pain!\n",
    "# The predictions and true values are effectively the same shape now (true values have another dimension, but it's size 1),\n",
    "# but we need to flatten them some more to get numpy to use memory effiently enough to do, well, math. \n",
    "\n",
    "best_keys_flat = best_keys.round().astype(int).flatten()\n",
    "y_keys_flat = y_keys.round().astype(int).flatten()\n",
    "\n",
    "best_texts_flat = best_texts.round().astype(int)\n",
    "y_texts_flat = y_texts[:,:,0].round().astype(int)\n",
    "\n",
    "# Keys are simple to compare now. How many were right?\n",
    "good_keys = (best_keys_flat == y_keys_flat).astype(int).sum()\n",
    "key_count = len(best_keys_flat)\n",
    "good_key_percent = float(good_keys) / float(key_count)\n",
    "\n",
    "# Text output requires some more assembly. Here is an example of how:\n",
    "sample_true_string = encoders.offsets_to_string(y_texts_flat[0,:])\n",
    "sample_predicted_string = encoders.offsets_to_string(best_texts_flat[0,:])\n",
    "print(\"A true string:\")\n",
    "print(f\"{sample_true_string[0: min(64,len(sample_true_string))]}...\")\n",
    "print()\n",
    "print(\"A predicted string:\")\n",
    "print(f\"{sample_predicted_string[0: min(64,len(sample_predicted_string))]}...\")\n",
    "\n",
    "good_chars = 0\n",
    "bad_chars = 0\n",
    "total_chars = 0\n",
    "for i in range(best_texts_flat.shape[0]):\n",
    "    true_string = encoders.offsets_to_string(y_texts_flat[i,:])\n",
    "    predicted_string = encoders.offsets_to_string(best_texts_flat[i,:])\n",
    "\n",
    "    # Count the matching characters\n",
    "    (this_good, this_bad, this_total, _) = helpers.good_bad_string_match(true_string, predicted_string)\n",
    "    good_chars += this_good\n",
    "    bad_chars += this_bad\n",
    "    total_chars += this_total\n",
    "good_text_percent = float(good_chars) / float(total_chars)\n",
    "\n",
    "print()\n",
    "print(f\"The key model produced the right key {good_key_percent:.2%} of the time.\")\n",
    "print(f\"The text model got {good_text_percent:.2%} of the characters right.\")\n",
    "\n",
    "# Add these to the Dataframe\n",
    "new_row = pd.DataFrame({\"Metric\": [\"Direct Accuracy\"], \"Key Model\": [good_key_percent], \"Text Model\": [good_text_percent]})\n",
    "evaluate_df = pd.concat([evaluate_df, new_row], ignore_index=True)\n",
    "evaluate_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
